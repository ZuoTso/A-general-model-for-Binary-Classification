{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import zscore, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y):\n",
    "    # clear missing value\n",
    "    missing_value = X.isnull().any(axis=1) | y.isnull().any(axis=1)\n",
    "    X, y = X[~missing_value], y[~missing_value]\n",
    "\n",
    "    # clear duplicate value\n",
    "    duplicate_value = pd.concat([X, y], axis=1).duplicated()\n",
    "    X, y = X[~duplicate_value], y[~duplicate_value]\n",
    "\n",
    "    # decide use minmax or zscore or yeojohnson\n",
    "    # it will pass the discrete feature\n",
    "    skewness = skew(X, axis=0)\n",
    "    transformers = []\n",
    "    for i, col in enumerate(X.columns):\n",
    "        if X[col].dtype == 'int64':\n",
    "            continue\n",
    "        elif np.abs(skewness[i]) > 1:\n",
    "            transformers.append((f'yeojohnson_{col}', PowerTransformer(method='yeo-johnson', standardize=False), [col]))\n",
    "        elif np.abs(skewness[i]) < 0.5:\n",
    "            transformers.append((f'standard_{col}', StandardScaler(), [col]))\n",
    "        else:\n",
    "            # clear outliers before minmax\n",
    "            z_score = np.abs(zscore(X[col]))\n",
    "            outliers = z_score > 3\n",
    "            X, y = X[~outliers], y[~outliers]\n",
    "            transformers.append((f'minmax_{col}', MinMaxScaler(), [col]))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers, remainder='passthrough') \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    X = pipeline.fit_transform(X)\n",
    "    y = y.to_numpy().ravel()\n",
    "    return X, y, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': 3,          # avoid overlapping\n",
    "    'weights': 'uniform',\n",
    "    'algorithm': 'auto',\n",
    "    'p': 2,\n",
    "}\n",
    "rf_params = {\n",
    "    'n_estimators': 100,      # fewer trees\n",
    "    'max_depth': 5,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': 42,\n",
    "}\n",
    "svc_params = {\n",
    "    'kernel': 'linear',\n",
    "    'C': 0.1,                  # avoid overlapping\n",
    "    'gamma': 'scale',          # avoid overlapping\n",
    "    'probability': True,\n",
    "    'random_state': 42,\n",
    "}\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,                 # L1 regularization\n",
    "    'reg_lambda': 0.1,                # L2 regularization\n",
    "    'scale_pos_weight': 1,            # Used for class imbalance problems\n",
    "    'random_state': 42,\n",
    "}\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 4,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l1': 0.1,                # L1 regularization\n",
    "    'lambda_l2': 0.1,                # L2 regularization\n",
    "    'max_bin': 255,\n",
    "    'scale_pos_weight': 1,           # Used for class imbalance problems\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "}\n",
    "meta_model_params = {\n",
    "    'solver': 'liblinear',\n",
    "    'C': 1.0,\n",
    "    'random_state': 42,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(X, y):\n",
    "    X, y, pipeline = preprocessing(X, y)\n",
    "\n",
    "    base_models = [\n",
    "        ('knn', KNeighborsClassifier(**knn_params)),\n",
    "        ('rf', RandomForestClassifier(**rf_params)),\n",
    "        ('svc', SVC(**svc_params)),\n",
    "        ('xgb', xgb.XGBClassifier(**xgb_params)),\n",
    "        ('lgb', lgb.LGBMClassifier(**lgb_params)),\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression(**meta_model_params)\n",
    "\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    )\n",
    "\n",
    "    stacking_model.fit(X, y)\n",
    "\n",
    "    return stacking_model, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_1 done\n",
      "Dataset_2 done\n",
      "Dataset_3 done\n",
      "Dataset_4 done\n",
      "Dataset_5 done\n",
      "Dataset_6 done\n",
      "Dataset_7 done\n",
      "Dataset_8 done\n",
      "Dataset_9 done\n",
      "Dataset_10 done\n",
      "Dataset_11 done\n",
      "Dataset_12 done\n",
      "Dataset_13 done\n",
      "Dataset_14 done\n",
      "Dataset_15 done\n",
      "Dataset_16 done\n",
      "Dataset_17 done\n",
      "Dataset_18 done\n",
      "Dataset_19 done\n",
      "Dataset_20 done\n",
      "Dataset_21 done\n",
      "Dataset_22 done\n",
      "Dataset_23 done\n",
      "Dataset_24 done\n",
      "Dataset_25 done\n",
      "Dataset_26 done\n",
      "Dataset_27 done\n",
      "Dataset_28 done\n",
      "Dataset_29 done\n",
      "Dataset_30 done\n",
      "Dataset_31 done\n",
      "Dataset_32 done\n",
      "Dataset_33 done\n",
      "Dataset_34 done\n",
      "Dataset_35 done\n",
      "Dataset_36 done\n",
      "Dataset_37 done\n",
      "Dataset_38 done\n",
      "Dataset_39 done\n",
      "Dataset_40 done\n",
      "Dataset_41 done\n",
      "Dataset_42 done\n",
      "Dataset_43 done\n",
      "Dataset_44 done\n",
      "Dataset_45 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cc980\\AppData\\Local\\Temp\\ipykernel_9664\\2229039062.py:11: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skewness = skew(X, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_46 done\n",
      "Dataset_47 done\n",
      "Dataset_48 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cc980\\AppData\\Local\\Temp\\ipykernel_9664\\2229039062.py:11: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skewness = skew(X, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_49 done\n"
     ]
    }
   ],
   "source": [
    "path = './Competition_data'\n",
    "dataset_names = os.listdir(path)\n",
    "dataset_names.sort(key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    X_train = pd.read_csv(f'./Competition_data/{dataset_name}/X_train.csv')\n",
    "    y_train = pd.read_csv(f'./Competition_data/{dataset_name}/y_train.csv')\n",
    "    \n",
    "    model, pipeline = processing(X_train, y_train)\n",
    "\n",
    "    X_test = pd.read_csv(f'./Competition_data/{dataset_name}/X_test.csv')\n",
    "    X_test = pipeline.transform(X_test)\n",
    "\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "\n",
    "    y_pred.columns = [\"y_predict_proba\"]\n",
    "\n",
    "    y_pred.to_csv(f'./Competition_data/{dataset_name}/y_predict.csv', index=False)\n",
    "\n",
    "    print(f\"{dataset_name} done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
